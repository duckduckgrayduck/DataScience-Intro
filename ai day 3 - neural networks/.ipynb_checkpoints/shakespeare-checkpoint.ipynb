{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e06003fd-b2dc-4b54-9a8e-b7b190c8c22d",
   "metadata": {},
   "source": [
    "# Generating Shakespeare-like text using Recurrent Neural Networks\n",
    "You will need to <b> pip install tensorflow. <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4276e2c-cd57-4492-9369-5176866f6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d5c84eb-4dbd-4cee-ace6-676931800489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1115394/1115394 [==============================] - 1s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset from Google's repo into tensorflow\n",
    "shakespeare_dataset = tf.keras.utils.get_file(\n",
    "    'shakespeare.txt',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4382311b-42d7-43f7-9db4-7f3b85233463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "text = open(shakespeare_dataset, 'rb').read().decode(encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b119cfa-35e4-461f-a68b-f470e343806e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61e53245-6f32-47b7-a013-a9965c41811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Identify unique characters in the dataset\n",
    "    Creates a sorted list (vocab) containing unique characters from the text.\n",
    "    Constructs a mapping (char_to_idx) from characters to their integer indices.\n",
    "    Converts the vocab list to a NumPy array (idx_to_char), enabling conversion from indices back to characters.\n",
    "'''\n",
    "vocab = sorted(set(text))\n",
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)} # Use to map characters in a word to their indices a=1, b=2, etc\n",
    "idx_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec5ac12e-efea-4d64-be68-470617661e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the entire text data into an array of integers using the mapping above. \n",
    "text_as_int = np.array([char_to_idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95318eb8-517f-4657-aead-298573f340a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)  # Creates a TF dataset from the integer representation of text.\n",
    "\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True) # Batches the dataset into sequences of length seq_length + 1 to use as training data.\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    ''' Splits each sequence into input and target sequences'''\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)  # Maps the function to the sequences dataset, generating pairs of input and target sequences.\n",
    "\n",
    "# Sets the batch size and prepares the dataset for training by batching it accordingly.\n",
    "batch_size = 64  # Adjust as needed\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00dec0e0-e131-4a04-a539-8581974ab71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "''' Sets up the architecture of the neural network using Keras' Sequential API.\n",
    "    Includes an embedding layer, a SimpleRNN layer, and a Dense (fully connected) layer.'''\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, batch_input_shape=[None, None]),\n",
    "    SimpleRNN(rnn_units, return_sequences=True),\n",
    "    Dense(vocab_size)\n",
    "])\n",
    "# Compiles the model with the Adam optimizer and Sparse Categorical Crossentropy loss function.\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02473146-775e-412b-ab71-3db654452c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 55s 314ms/step - loss: 2.7579\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 56s 324ms/step - loss: 2.2163\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 56s 324ms/step - loss: 2.0080\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 56s 324ms/step - loss: 1.8795\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 55s 320ms/step - loss: 1.7805\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 55s 318ms/step - loss: 1.7022\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 54s 312ms/step - loss: 1.6406\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 53s 310ms/step - loss: 1.5916\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 52s 302ms/step - loss: 1.5521\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 55s 321ms/step - loss: 1.5189\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 56s 323ms/step - loss: 1.4903\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 52s 305ms/step - loss: 1.4662\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 54s 317ms/step - loss: 1.4448\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 56s 325ms/step - loss: 1.4266\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 59s 343ms/step - loss: 1.4116\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 72s 420ms/step - loss: 1.4021\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 71s 413ms/step - loss: 1.3906\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 55s 323ms/step - loss: 1.3771\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 54s 317ms/step - loss: 1.3659\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 50s 293ms/step - loss: 1.3567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f7a19b45cf0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sets up a callback to save the model weights during training.\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='./shakespeare_model_checkpoint.h5', save_weights_only=True)\n",
    "\n",
    "# Trains the model for 20 epochs using the prepared dataset and saves the model weights at the end of each epoch.\n",
    "model.fit(dataset, epochs=20, callbacks=[checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "513e50e9-aa7f-4310-925a-dbea33995b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: I bumy ind s me mat thes st t myo at an an thinor ful s br the houndes be burer these s se bou oo mor t my ho al te d thestounthoow a myoupe and mel me s d t alinghe inooul se thand anow ht mes bl th hyo wen m this my ber mat, myo thand mis! me thanoous wartheast whe d mous\n",
      "HARIO:\n",
      "\n",
      "TI r boreroust he he bl the the me ino my theel th athowillongeat t fe l thor me fo w thot th an at ge atengous he hinor we he t bino d d me te th ming inont me my.\n",
      "BERIO:\n",
      "I fo anof t me houre fel th be myor ho he in \n"
     ]
    }
   ],
   "source": [
    "# Restore the latest checkpoint\n",
    "model.load_weights('./shakespeare_model_checkpoint.h5')\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "def generate_text(model, start_string):\n",
    "    num_generate = 500\n",
    "    input_eval = [char_to_idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = []\n",
    "    \n",
    "    temperature = 0.5\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx_to_char[predicted_id])\n",
    "        \n",
    "    return (start_string + ''.join(text_generated))\n",
    "\n",
    "print(generate_text(model, start_string=\"ROMEO: \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1359973f-45bb-48e1-b32d-9113dbec93ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
